{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Dropbox\\\\Self-Development\\\\Coding_Projects\\\\TextSummarizer'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    model_ckpt: Path\n",
    "    num_train_epochs: int\n",
    "    warmup_steps: int\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    weight_decay: float\n",
    "    logging_steps: int\n",
    "    evaluation_strategy: str\n",
    "    eval_steps: int\n",
    "    save_steps: int\n",
    "    gradient_accumulation_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.exception import CustomException\n",
    "from src.utils.logger import logger\n",
    "from src.utils.utils import load_yaml, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = load_yaml(config_filepath)\n",
    "        self.params = load_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config[\"artifacts_root\"]])\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config[\"model_trainer\"]\n",
    "        params = self.params[\"TrainingArguments\"]\n",
    "        \n",
    "        create_directories([config[\"root_dir\"]])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config[\"root_dir\"],\n",
    "            data_path=config[\"data_path\"],\n",
    "            model_ckpt=config[\"model_ckpt\"],\n",
    "            num_train_epochs=params[\"num_train_epochs\"],\n",
    "            warmup_steps=params[\"warmup_steps\"],\n",
    "            per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "            per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "            weight_decay=params[\"weight_decay\"],\n",
    "            logging_steps=params[\"logging_steps\"],\n",
    "            evaluation_strategy=params[\"evaluation_strategy\"],\n",
    "            eval_steps=params[\"eval_steps\"],\n",
    "            save_steps=params[\"save_steps\"],\n",
    "            gradient_accumulation_steps=params[\"gradient_accumulation_steps\"]\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from src.utils.exception import CustomException\n",
    "from src.utils.logger import logger\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def train(self):\n",
    "        try:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n",
    "            logger.info(\"Tokenizer initialized\")\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "            logger.info(\"Model initialized\")\n",
    "            data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "            logger.info(\"Data Collator initialized\")\n",
    "            \n",
    "            #loading data \n",
    "            dataset_transformed = load_from_disk(self.config.data_path)\n",
    "            logger.info(\"Dataset loaded\")\n",
    "\n",
    "            trainer_args = TrainingArguments(\n",
    "                output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n",
    "                per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_eval_batch_size,\n",
    "                weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n",
    "                evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(model=model, args=trainer_args,\n",
    "                    tokenizer=tokenizer, data_collator=data_collator,\n",
    "                    train_dataset=dataset_transformed[\"train\"],\n",
    "                    eval_dataset=dataset_transformed[\"validation\"])\n",
    "            logger.info(\"Trainer initialized\")\n",
    "            \n",
    "            trainer.train()\n",
    "            logger.info(\"Training process finished\")\n",
    "\n",
    "            ## Save model\n",
    "            model.save_pretrained(os.path.join(self.config.root_dir,\"trained-model\"))\n",
    "            logger.info(\"Trained model saved\")\n",
    "            \n",
    "            ## Save tokenizer\n",
    "            tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))\n",
    "            logger.info(\"Tokenizer saved\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 00:17:27,344: INFO: utils: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-08-07 00:17:27,347: INFO: utils: yaml file: params.yaml loaded successfully]\n",
      "[2023-08-07 00:17:27,349: INFO: utils: created directory at: artifacts]\n",
      "[2023-08-07 00:17:27,349: INFO: utils: created directory at: artifacts/model_trainer]\n",
      "[2023-08-07 00:17:28,095: INFO: 1905566313: Tokenizer initialized]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 00:17:37,672: INFO: 1905566313: Model initialized]\n",
      "[2023-08-07 00:17:37,673: INFO: 1905566313: Data Collator initialized]\n",
      "[2023-08-07 00:17:37,700: INFO: 1905566313: Dataset loaded]\n",
      "[2023-08-07 00:17:38,813: INFO: 1905566313: Trainer initialized]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 00:17:39,453: ERROR: jupyter: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjjjoooo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\wandb\\run-20230807_001741-pm98enj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jjjoooo/huggingface/runs/pm98enj6' target=\"_blank\">clean-shape-18</a></strong> to <a href='https://wandb.ai/jjjoooo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jjjoooo/huggingface' target=\"_blank\">https://wandb.ai/jjjoooo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jjjoooo/huggingface/runs/pm98enj6' target=\"_blank\">https://wandb.ai/jjjoooo/huggingface/runs/pm98enj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "CustomException",
     "evalue": "Error occurred in script name [C:\\Users\\jinou\\AppData\\Local\\Temp\\ipykernel_122116\\1905566313.py] line number [43] error message [CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 8.00 GiB total capacity; 6.91 GiB already allocated; 0 bytes free; 7.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 43\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTrainer initialized\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     44\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTraining process finished\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:1809\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1815\u001b[0m ):\n\u001b[0;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\transformers\\trainer.py:2665\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[0;32m   2667\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\accelerate\\accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1853\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Dropbox\\Self-Development\\Coding_Projects\\TextSummarizer\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 8.00 GiB total capacity; 6.91 GiB already allocated; 0 bytes free; 7.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model_trainer_config\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget_model_trainer_config()\n\u001b[0;32m      4\u001b[0m     model_trainer_config \u001b[39m=\u001b[39m ModelTrainer(config\u001b[39m=\u001b[39mmodel_trainer_config)\n\u001b[1;32m----> 5\u001b[0m     model_trainer_config\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m      6\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[21], line 55\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTokenizer saved\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m CustomException(e, sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Error occurred in script name [C:\\Users\\jinou\\AppData\\Local\\Temp\\ipykernel_122116\\1905566313.py] line number [43] error message [CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 8.00 GiB total capacity; 6.91 GiB already allocated; 0 bytes free; 7.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF]"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer_config = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer_config.train()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
